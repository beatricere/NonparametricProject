{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importo le librerie\n",
    "import pyreadr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Caricamento del dataset\n",
    "result = pyreadr.read_r(\"../Data/dataset_final.rds\")\n",
    "data = result[None]\n",
    "\n",
    "# Seleziono le feature e la variabile target\n",
    "selected_columns = ['neur', 'lymr', 'monor', 'crea', 'age', 'bun', 'crp', 'wbc']  # METTERE QUI I NOMI DELLE COLONNE DA INCLUDERE\n",
    "features = data[selected_columns]\n",
    "target = data['bacteremia']\n",
    "\n",
    "# Normalizzo le feature\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Divido il dataset: training set (solo classe 0) e test set (tutti i dati)\n",
    "X_train = X_scaled[target == 0]  # Addestriamo l'autoencoder solo con classe 0\n",
    "X_test = X_scaled  # Usiamo tutti i dati per il test\n",
    "y_test = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,032</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m1,152\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │         \u001b[38;5;34m1,032\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,024</span> (93.84 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,024\u001b[0m (93.84 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,024</span> (93.84 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m24,024\u001b[0m (93.84 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Costruzione dell'autoencoder\n",
    "input_dim = X_train.shape[1]  # Numero di feature\n",
    "\n",
    "# # Definisco la rete neurale\n",
    "# input_layer = Input(shape=(input_dim,))\n",
    "# encoded = Dense(8, activation='relu')(input_layer)   # Primo livello nascosto\n",
    "# encoded = Dense(4, activation='relu')(encoded)       # Bottleneck (compressione)\n",
    "# decoded = Dense(8, activation='relu')(encoded)       # Primo livello di decodifica\n",
    "# decoded = Dense(input_dim, activation='linear')(decoded)  # Output layer\n",
    "\n",
    "# # Creo il modello autoencoder\n",
    "# autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "\n",
    "# # Compilo il modello\n",
    "# autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "\n",
    "# PROVO UN'ARCHITETTURA PIU' COMPLESSA:\n",
    "\n",
    "# Encoder\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(128, activation='relu', activity_regularizer=l2(1e-5))(input_layer)\n",
    "encoded = Dropout(0.2)(encoded)  # Dropout al 20%\n",
    "encoded = Dense(64, activation='relu', activity_regularizer=l2(1e-5))(encoded)\n",
    "encoded = Dense(32, activation='relu', activity_regularizer=l2(1e-5))(encoded)\n",
    "\n",
    "# Bottleneck\n",
    "bottleneck = Dense(16, activation='relu', activity_regularizer=l2(1e-5))(encoded)\n",
    "\n",
    "# Decoder\n",
    "decoded = Dense(32, activation='relu')(bottleneck)\n",
    "decoded = Dense(64, activation='relu')(decoded)\n",
    "decoded = Dropout(0.2)(decoded)  # Dropout al 20%\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "output_layer = Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "# Modello\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compilazione\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Stampa il sommario della rete\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 0.5460 - val_loss: 0.0879\n",
      "Epoch 2/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1596 - val_loss: 0.0481\n",
      "Epoch 3/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1095 - val_loss: 0.0551\n",
      "Epoch 4/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0926 - val_loss: 0.0517\n",
      "Epoch 5/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.1241 - val_loss: 0.0309\n",
      "Epoch 6/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0783 - val_loss: 0.0940\n",
      "Epoch 7/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0984 - val_loss: 0.0381\n",
      "Epoch 8/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0675 - val_loss: 0.0487\n",
      "Epoch 9/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0813 - val_loss: 0.0274\n",
      "Epoch 10/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0794 - val_loss: 0.0226\n",
      "Epoch 11/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0638 - val_loss: 0.0462\n",
      "Epoch 12/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0628 - val_loss: 0.0298\n",
      "Epoch 13/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0657 - val_loss: 0.0272\n",
      "Epoch 14/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0602 - val_loss: 0.0696\n",
      "Epoch 15/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0854 - val_loss: 0.0254\n",
      "Epoch 16/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0551 - val_loss: 0.0276\n",
      "Epoch 17/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0534 - val_loss: 0.0240\n",
      "Epoch 18/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0851 - val_loss: 0.0372\n",
      "Epoch 19/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0665 - val_loss: 0.0474\n",
      "Epoch 20/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0675 - val_loss: 0.0676\n",
      "Epoch 21/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0709 - val_loss: 0.0734\n",
      "Epoch 22/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0828 - val_loss: 0.0357\n",
      "Epoch 23/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0566 - val_loss: 0.0342\n",
      "Epoch 24/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0534 - val_loss: 0.0233\n",
      "Epoch 25/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0620 - val_loss: 0.0313\n",
      "Epoch 26/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0636 - val_loss: 0.0822\n",
      "Epoch 27/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0635 - val_loss: 0.0323\n",
      "Epoch 28/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0572 - val_loss: 0.0291\n",
      "Epoch 29/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0504 - val_loss: 0.0309\n",
      "Epoch 30/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0503 - val_loss: 0.0334\n",
      "Epoch 31/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0461 - val_loss: 0.0300\n",
      "Epoch 32/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0435 - val_loss: 0.0352\n",
      "Epoch 33/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0429 - val_loss: 0.0652\n",
      "Epoch 34/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0504 - val_loss: 0.0620\n",
      "Epoch 35/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0571 - val_loss: 0.0468\n",
      "Epoch 36/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0535 - val_loss: 0.0309\n",
      "Epoch 37/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0471 - val_loss: 0.0303\n",
      "Epoch 38/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0419 - val_loss: 0.0539\n",
      "Epoch 39/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0422 - val_loss: 0.0398\n",
      "Epoch 40/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0422 - val_loss: 0.0448\n",
      "Epoch 41/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0421 - val_loss: 0.0482\n",
      "Epoch 42/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0473 - val_loss: 0.1218\n",
      "Epoch 43/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0857 - val_loss: 0.0495\n",
      "Epoch 44/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0433 - val_loss: 0.0503\n",
      "Epoch 45/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0419 - val_loss: 0.0449\n",
      "Epoch 46/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0633 - val_loss: 0.0408\n",
      "Epoch 47/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0415 - val_loss: 0.0418\n",
      "Epoch 48/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0368 - val_loss: 0.0580\n",
      "Epoch 49/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0491 - val_loss: 0.0448\n",
      "Epoch 50/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0355 - val_loss: 0.0630\n",
      "Epoch 51/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0449 - val_loss: 0.0424\n",
      "Epoch 52/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0353 - val_loss: 0.0585\n",
      "Epoch 53/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0359 - val_loss: 0.0470\n",
      "Epoch 54/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0499 - val_loss: 0.0516\n",
      "Epoch 55/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0521 - val_loss: 0.0511\n",
      "Epoch 56/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0365 - val_loss: 0.0599\n",
      "Epoch 57/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0457 - val_loss: 0.0769\n",
      "Epoch 58/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0621 - val_loss: 0.0548\n",
      "Epoch 59/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0490 - val_loss: 0.0537\n",
      "Epoch 60/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0412 - val_loss: 0.0569\n",
      "Epoch 61/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0349 - val_loss: 0.0697\n",
      "Epoch 62/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0399 - val_loss: 0.0741\n",
      "Epoch 63/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0686 - val_loss: 0.0606\n",
      "Epoch 64/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0350 - val_loss: 0.0716\n",
      "Epoch 65/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0319 - val_loss: 0.0609\n",
      "Epoch 66/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0331 - val_loss: 0.0631\n",
      "Epoch 67/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0306 - val_loss: 0.0729\n",
      "Epoch 68/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0328 - val_loss: 0.0611\n",
      "Epoch 69/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0491 - val_loss: 0.0669\n",
      "Epoch 70/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0342 - val_loss: 0.0688\n",
      "Epoch 71/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0342 - val_loss: 0.0675\n",
      "Epoch 72/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0308 - val_loss: 0.0692\n",
      "Epoch 73/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0617 - val_loss: 0.0697\n",
      "Epoch 74/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0348 - val_loss: 0.0887\n",
      "Epoch 75/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0280 - val_loss: 0.0816\n",
      "Epoch 76/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0336 - val_loss: 0.0749\n",
      "Epoch 77/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0385 - val_loss: 0.0704\n",
      "Epoch 78/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0405 - val_loss: 0.0759\n",
      "Epoch 79/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0327 - val_loss: 0.0730\n",
      "Epoch 80/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0430 - val_loss: 0.0772\n",
      "Epoch 81/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0468 - val_loss: 0.0782\n",
      "Epoch 82/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0324 - val_loss: 0.0861\n",
      "Epoch 83/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0328 - val_loss: 0.0846\n",
      "Epoch 84/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0302 - val_loss: 0.0986\n",
      "Epoch 85/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0307 - val_loss: 0.0812\n",
      "Epoch 86/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0332 - val_loss: 0.0700\n",
      "Epoch 87/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0316 - val_loss: 0.0807\n",
      "Epoch 88/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0283 - val_loss: 0.0768\n",
      "Epoch 89/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0471 - val_loss: 0.0858\n",
      "Epoch 90/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0556 - val_loss: 0.0988\n",
      "Epoch 91/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0344 - val_loss: 0.1029\n",
      "Epoch 92/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0311 - val_loss: 0.0962\n",
      "Epoch 93/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0275 - val_loss: 0.0840\n",
      "Epoch 94/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0270 - val_loss: 0.0948\n",
      "Epoch 95/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0260 - val_loss: 0.0986\n",
      "Epoch 96/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0335 - val_loss: 0.0990\n",
      "Epoch 97/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0334 - val_loss: 0.1080\n",
      "Epoch 98/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0341 - val_loss: 0.1003\n",
      "Epoch 99/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0427 - val_loss: 0.1002\n",
      "Epoch 100/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0478 - val_loss: 0.0884\n",
      "Epoch 101/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0268 - val_loss: 0.0886\n",
      "Epoch 102/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0306 - val_loss: 0.0950\n",
      "Epoch 103/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0375 - val_loss: 0.1129\n",
      "Epoch 104/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0265 - val_loss: 0.0964\n",
      "Epoch 105/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0253 - val_loss: 0.1066\n",
      "Epoch 106/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0285 - val_loss: 0.1034\n",
      "Epoch 107/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0337 - val_loss: 0.0980\n",
      "Epoch 108/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0263 - val_loss: 0.1030\n",
      "Epoch 109/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0281 - val_loss: 0.1003\n",
      "Epoch 110/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0332 - val_loss: 0.0946\n",
      "Epoch 111/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0275 - val_loss: 0.0989\n",
      "Epoch 112/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0256 - val_loss: 0.0941\n",
      "Epoch 113/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0283 - val_loss: 0.0955\n",
      "Epoch 114/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0276 - val_loss: 0.0927\n",
      "Epoch 115/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0247 - val_loss: 0.0895\n",
      "Epoch 116/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0307 - val_loss: 0.1019\n",
      "Epoch 117/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0426 - val_loss: 0.0978\n",
      "Epoch 118/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0334 - val_loss: 0.0978\n",
      "Epoch 119/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0304 - val_loss: 0.1032\n",
      "Epoch 120/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0268 - val_loss: 0.1073\n",
      "Epoch 121/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0266 - val_loss: 0.1038\n",
      "Epoch 122/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0513 - val_loss: 0.1078\n",
      "Epoch 123/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0295 - val_loss: 0.1007\n",
      "Epoch 124/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0233 - val_loss: 0.1071\n",
      "Epoch 125/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0259 - val_loss: 0.1162\n",
      "Epoch 126/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0390 - val_loss: 0.1268\n",
      "Epoch 127/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0289 - val_loss: 0.1076\n",
      "Epoch 128/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0303 - val_loss: 0.1037\n",
      "Epoch 129/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0290 - val_loss: 0.1222\n",
      "Epoch 130/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0285 - val_loss: 0.1438\n",
      "Epoch 131/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0323 - val_loss: 0.1121\n",
      "Epoch 132/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0271 - val_loss: 0.1116\n",
      "Epoch 133/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0241 - val_loss: 0.0995\n",
      "Epoch 134/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0226 - val_loss: 0.1089\n",
      "Epoch 135/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0267 - val_loss: 0.1200\n",
      "Epoch 136/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0241 - val_loss: 0.1106\n",
      "Epoch 137/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0250 - val_loss: 0.1185\n",
      "Epoch 138/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0331 - val_loss: 0.1125\n",
      "Epoch 139/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0234 - val_loss: 0.1024\n",
      "Epoch 140/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0410 - val_loss: 0.1168\n",
      "Epoch 141/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0255 - val_loss: 0.1173\n",
      "Epoch 142/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0282 - val_loss: 0.1154\n",
      "Epoch 143/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0252 - val_loss: 0.1227\n",
      "Epoch 144/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0260 - val_loss: 0.1216\n",
      "Epoch 145/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0234 - val_loss: 0.1207\n",
      "Epoch 146/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0269 - val_loss: 0.1274\n",
      "Epoch 147/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0237 - val_loss: 0.1122\n",
      "Epoch 148/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0227 - val_loss: 0.1154\n",
      "Epoch 149/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0237 - val_loss: 0.1289\n",
      "Epoch 150/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0330 - val_loss: 0.1198\n",
      "Epoch 151/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0223 - val_loss: 0.1223\n",
      "Epoch 152/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0267 - val_loss: 0.1136\n",
      "Epoch 153/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0242 - val_loss: 0.1072\n",
      "Epoch 154/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0221 - val_loss: 0.1378\n",
      "Epoch 155/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0373 - val_loss: 0.1153\n",
      "Epoch 156/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0322 - val_loss: 0.1108\n",
      "Epoch 157/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0247 - val_loss: 0.1181\n",
      "Epoch 158/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0209 - val_loss: 0.1247\n",
      "Epoch 159/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0230 - val_loss: 0.1218\n",
      "Epoch 160/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0277 - val_loss: 0.1507\n",
      "Epoch 161/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0391 - val_loss: 0.1128\n",
      "Epoch 162/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0231 - val_loss: 0.1291\n",
      "Epoch 163/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0213 - val_loss: 0.1251\n",
      "Epoch 164/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0211 - val_loss: 0.1196\n",
      "Epoch 165/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0222 - val_loss: 0.1282\n",
      "Epoch 166/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0231 - val_loss: 0.1332\n",
      "Epoch 167/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0267 - val_loss: 0.1175\n",
      "Epoch 168/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0341 - val_loss: 0.1276\n",
      "Epoch 169/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0239 - val_loss: 0.1294\n",
      "Epoch 170/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0234 - val_loss: 0.1224\n",
      "Epoch 171/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0223 - val_loss: 0.1301\n",
      "Epoch 172/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0250 - val_loss: 0.1340\n",
      "Epoch 173/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0223 - val_loss: 0.1283\n",
      "Epoch 174/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0240 - val_loss: 0.1314\n",
      "Epoch 175/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0215 - val_loss: 0.1335\n",
      "Epoch 176/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0317 - val_loss: 0.1284\n",
      "Epoch 177/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0433 - val_loss: 0.1218\n",
      "Epoch 178/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0219 - val_loss: 0.1386\n",
      "Epoch 179/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0199 - val_loss: 0.1274\n",
      "Epoch 180/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0250 - val_loss: 0.1193\n",
      "Epoch 181/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0234 - val_loss: 0.1350\n",
      "Epoch 182/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0341 - val_loss: 0.1204\n",
      "Epoch 183/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0221 - val_loss: 0.1285\n",
      "Epoch 184/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0310 - val_loss: 0.1317\n",
      "Epoch 185/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0287 - val_loss: 0.1263\n",
      "Epoch 186/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0213 - val_loss: 0.1348\n",
      "Epoch 187/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0221 - val_loss: 0.1406\n",
      "Epoch 188/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0343 - val_loss: 0.1374\n",
      "Epoch 189/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0224 - val_loss: 0.1372\n",
      "Epoch 190/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0232 - val_loss: 0.1495\n",
      "Epoch 191/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0314 - val_loss: 0.1406\n",
      "Epoch 192/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0192 - val_loss: 0.1384\n",
      "Epoch 193/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0269 - val_loss: 0.1267\n",
      "Epoch 194/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0331 - val_loss: 0.1363\n",
      "Epoch 195/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0307 - val_loss: 0.1313\n",
      "Epoch 196/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0219 - val_loss: 0.1459\n",
      "Epoch 197/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0241 - val_loss: 0.1670\n",
      "Epoch 198/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0295 - val_loss: 0.1236\n",
      "Epoch 199/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0264 - val_loss: 0.1242\n",
      "Epoch 200/200\n",
      "\u001b[1m366/366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0255 - val_loss: 0.1247\n"
     ]
    }
   ],
   "source": [
    "# 3. Addestramento del modello\n",
    "history = autoencoder.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m441/441\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Soglia errore per rilevare anomalie: 0.42576\n"
     ]
    }
   ],
   "source": [
    "# 4. Valutazione del modello\n",
    "# Ricostruisco il test set\n",
    "X_test_pred = autoencoder.predict(X_test)\n",
    "\n",
    "# Calcolo l'errore di ricostruzione\n",
    "reconstruction_error = np.mean((X_test - X_test_pred) ** 2, axis=1)\n",
    "\n",
    "# Definisco una soglia per identificare le anomalie (classe 1)\n",
    "threshold = np.percentile(reconstruction_error, 95)  # Soglia al 95° percentile\n",
    "print(f\"Soglia errore per rilevare anomalie: {threshold:.5f}\")\n",
    "\n",
    "# Classifico le anomalie\n",
    "y_pred = (reconstruction_error > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribuzione degli errori di ricostruzione\n",
    "plt.hist(reconstruction_error[y_test == 0], bins=50, alpha=0.5, label='Classe 0')\n",
    "plt.hist(reconstruction_error[y_test == 1], bins=50, alpha=0.5, label='Classe 1')\n",
    "plt.legend()\n",
    "plt.xlabel('Errore di Ricostruzione')\n",
    "plt.ylabel('Frequenza')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.5095\n",
      "Precision: 0.1147\n",
      "Recall: 0.0719\n",
      "F1-Score: 0.0884\n",
      "Confusion Matrix:\n",
      "[[12355   625]\n",
      " [ 1046    81]]\n"
     ]
    }
   ],
   "source": [
    "# 5. Valutazione delle performance\n",
    "auc = roc_auc_score(y_test, reconstruction_error)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"AUC-ROC: {auc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IL MODELLO ATTUALMENTE FA SCHIFO! Possibili miglioramenti:\n",
    "1) cambiare features considerate (aumentarle?)\n",
    "2) modificare training e test set?\n",
    "3) modificare architettura della rete (numero layers, numero neuroni, activation function)\n",
    "4) cambiare optimizer (ora adam) o learning rate e/o definizione loss\n",
    "5) cambiare numero di epoche\n",
    "6) calibrare soglia per definire le anomalie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
